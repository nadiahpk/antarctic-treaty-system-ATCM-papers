# In the LLM prompt, the LLM was instructed to mark uncertain 
# characters with "[?]". Helpfully, the LLM decided to sometimes
# mark uncertain characters with a "?" instead. So I'll have to 
# apply some heuristics.
#
# 1. count all "?" and include that in our data for baseline comparison
# 2. count all "[?]" as definitely illegible
# 3. Apply heuristics to identify likely illegible "?" markers:
#    - "?" within alphanumeric sequences (no spaces on either side)
#    - Multiple "?" in sequence (??, ???, etc.)
#    - "?" at the start of a word followed by letters
#
# For each document, count the number of "?" as well as 
# definitely-illegible characters as a proportion of the total number 
# of characters in the document. This will give us a conservative
# (i.e., overestimate) of the proportion of characters the AI couldn't
# decipher.

import os
from pathlib import Path
import re

import pandas as pd


# parameters
# ---

# location of the data (extracted text files)
data_base = Path(os.environ.get("DATA", "."))

# number of example unknown words to include in CSV
# (most have only a handful and this allows me to quickly check)
nbr_example_unknown_words = 50

# identifies which text files were generated by AI
fin_ai = Path("../results_describe_data/which_OCR_with_AI.csv")


# get list of AI-generated text files, may have illegibility marks
# ---

df = pd.read_csv(
    fin_ai,
    usecols=[
        "path", # location of file relative to data_base
        "OCR_with_AI", # True/False
    ]
)
assert(set(df["OCR_with_AI"]) == {True, False})


# get all files that were OCR'd with AI
# ---

txt_path_strs = list(df[df["OCR_with_AI"]]["path"])
txt_paths = [
    data_base / txt_path_str
    for txt_path_str in txt_path_strs
]
print(f"{len(txt_paths)} text files found that were OCR'd by AI")
# -- for this dataset, it's 2082


# for each file, count the total number of characters 
# and the number of [?]
# ---

# storage
nbr_charV = list()
nbr_bracketed_illegibleV = list()  # [?] count
nbr_total_qmarksV = list()  # all ? count (for baseline)
nbr_likely_illegible_qmarksV = list()  # ? that match our heuristics
propn_total_illegibleV = list()  # proportion of all likely illegible
eg_bracketed_illegibleV = list()
eg_likely_illegible_qmarksV = list()

for txt_path in txt_paths:
    # read in text 
    text = txt_path.read_text(encoding="utf-8")

    # count total characters
    nbr_char = len(text)
    
    eg_bracketed_illegible = set()
    eg_likely_illegible_qmarks = set()
    
    if nbr_char > 0:
        # count instances of [?]
        nbr_bracketed_illegible = text.count('[?]')
        
        # count total instances of ? (including those in [?])
        nbr_total_qmarks = text.count('?')
        
        # Apply heuristics to find likely illegible ? markers
        # We'll find all positions of likely illegible ? to avoid double counting
        likely_illegible_positions = set()
        
        # Pattern 1: ? within alphanumeric sequences (no spaces on either side)
        for match in re.finditer(r'[^\s]\?[^\s]', text):
            if '[?' not in match.group() and '?]' not in match.group():
                likely_illegible_positions.add(match.start() + 1)  # position of ?
        
        # Pattern 2: Multiple ? in sequence
        for match in re.finditer(r'\?{2,}', text):
            for i in range(match.start(), match.end()):
                likely_illegible_positions.add(i)
        
        # Pattern 3: ? at start of word followed by letters
        for match in re.finditer(r'\b\?[a-zA-Z]+', text):
            likely_illegible_positions.add(match.start())
        
        # Total count of likely illegible ?
        nbr_likely_illegible_qmarks = len(likely_illegible_positions)
        
        # Collect examples of likely illegible ? - find complete words containing these positions
        if nbr_likely_illegible_qmarks > 0:
            # Find all words containing ?
            words_with_qmarks = re.findall(r'\S*\?\S*', text)
            for word in words_with_qmarks:
                # Check if this word contains any of our likely illegible positions
                word_start = text.find(word)
                if word_start != -1:
                    word_positions = set(range(word_start, word_start + len(word)))
                    if word_positions & likely_illegible_positions:  # intersection
                        if '[?' not in word and '?]' not in word:  # exclude bracketed ones
                            eg_likely_illegible_qmarks.add(word)
                            if len(eg_likely_illegible_qmarks) >= nbr_example_unknown_words:
                                break
        
        # Total illegible = [?] + likely illegible ?
        nbr_total_illegible = nbr_bracketed_illegible + nbr_likely_illegible_qmarks
        
        # Adjust character count (subtract 2 chars for each [?] since we count the content, not the brackets)
        adjusted_nbr_char = nbr_char - 2 * nbr_bracketed_illegible
        
        # Calculate proportions
        if adjusted_nbr_char > 0:
            propn_total_illegible = nbr_total_illegible / adjusted_nbr_char
        else:
            propn_total_illegible = 0
        
        # Collect examples of bracketed illegible
        if nbr_bracketed_illegible > 0:
            words_with_bracketed = re.findall(r'\S*\[\?\]\S*', text)
            for word in words_with_bracketed[:nbr_example_unknown_words]:
                eg_bracketed_illegible.add(word)
                
    else:
        # default values for empty docs
        nbr_bracketed_illegible = 0
        nbr_total_qmarks = 0
        nbr_likely_illegible_qmarks = 0
        nbr_total_illegible = 0
        propn_total_illegible = 0
        adjusted_nbr_char = 0

    # store
    # ---
    nbr_charV.append(adjusted_nbr_char)
    nbr_bracketed_illegibleV.append(nbr_bracketed_illegible)
    nbr_total_qmarksV.append(nbr_total_qmarks)
    nbr_likely_illegible_qmarksV.append(nbr_likely_illegible_qmarks)
    propn_total_illegibleV.append(propn_total_illegible)
    eg_bracketed_illegibleV.append(" | ".join(list(eg_bracketed_illegible)[:nbr_example_unknown_words]))
    eg_likely_illegible_qmarksV.append(" | ".join(list(eg_likely_illegible_qmarks)[:nbr_example_unknown_words]))

# write info to a CSV
# ---

txt_rel_paths = [path.relative_to(data_base) for path in txt_paths]
df = pd.DataFrame(
    {
        "path": txt_rel_paths,
        "nbr_characters": nbr_charV,
        "nbr_bracketed_illegible": nbr_bracketed_illegibleV,
        "nbr_total_qmarks": nbr_total_qmarksV,
        "nbr_likely_illegible_qmarks": nbr_likely_illegible_qmarksV,
        "propn_total_illegible": propn_total_illegibleV,
        "example_bracketed_illegible": eg_bracketed_illegibleV,
        "example_likely_illegible_qmarks": eg_likely_illegible_qmarksV,
    }
)

# Create output directory and save
os.makedirs("../results_describe_data", exist_ok=True)
df.to_csv("../results_describe_data/count_illegible_ocr.csv", index=False)